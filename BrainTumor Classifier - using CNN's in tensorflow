{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2236708,"sourceType":"datasetVersion","datasetId":1343913}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-11T21:21:38.074996Z","iopub.execute_input":"2024-06-11T21:21:38.076009Z","iopub.status.idle":"2024-06-11T21:21:38.082721Z","shell.execute_reply.started":"2024-06-11T21:21:38.075965Z","shell.execute_reply":"2024-06-11T21:21:38.081214Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# The two classes we are classifying\nds_dir = '/kaggle/input/brian-tumor-dataset/Brain Tumor Data Set/Brain Tumor Data Set'\nos.listdir(ds_dir)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:24:46.186185Z","iopub.execute_input":"2024-06-11T20:24:46.186686Z","iopub.status.idle":"2024-06-11T20:24:46.199016Z","shell.execute_reply.started":"2024-06-11T20:24:46.186646Z","shell.execute_reply":"2024-06-11T20:24:46.197853Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['Brain Tumor', 'Healthy']"},"metadata":{}}]},{"cell_type":"code","source":"# Reading the csv files\n\nmetadata_df = pd.read_csv('/kaggle/input/brian-tumor-dataset/metadata.csv')\nmetadata_df.head()\n# dropping the 'unnamed: 0' column to make our dataset neat\nmetadata_df.drop(columns='Unnamed: 0', inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:32:59.559222Z","iopub.execute_input":"2024-06-11T20:32:59.559701Z","iopub.status.idle":"2024-06-11T20:32:59.589063Z","shell.execute_reply.started":"2024-06-11T20:32:59.559666Z","shell.execute_reply":"2024-06-11T20:32:59.587706Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nThe metadata file contains label information about each image with our dataset\n\nObservations:\n============\n\n1. Class column has two classes -> [Tumor, Healthy]\n2. The image formats are varying -> ['PNG', 'TIFF', 'JPEG']\n3. The image mode, we basically have two image modes -> ['RGB', 'Grayscale(L)']\n4. The varying image sizes per image with in the dataset.\n\nWay Forward:\n============\n1. maintaining a single image format might be crucial to the robustness of our model,\nusing 'PNG' format per say -> retains more details due to lossless compressions\n2. RGB or Grayscale as RGB images often capture more information with in the image.\n3. Using (224,224) as our image size -> Besides increase in computational efficiency, there\nis increased ability to capture key features\n\"\"\"\nprint(metadata_df.head().to_markdown())","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:50:33.731427Z","iopub.execute_input":"2024-06-11T20:50:33.732268Z","iopub.status.idle":"2024-06-11T20:50:33.742713Z","shell.execute_reply.started":"2024-06-11T20:50:33.732223Z","shell.execute_reply":"2024-06-11T20:50:33.741233Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"|    | image           | class   | format   | mode   | shape         |\n|---:|:----------------|:--------|:---------|:-------|:--------------|\n|  0 | Cancer (1).jpg  | tumor   | JPEG     | RGB    | (512, 512, 3) |\n|  1 | Cancer (1).png  | tumor   | PNG      | L      | (300, 240)    |\n|  2 | Cancer (1).tif  | tumor   | TIFF     | RGB    | (256, 256, 3) |\n|  3 | Cancer (10).jpg | tumor   | JPEG     | RGB    | (512, 512, 3) |\n|  4 | Cancer (10).tif | tumor   | TIFF     | RGB    | (256, 256, 3) |\n","output_type":"stream"}]},{"cell_type":"code","source":"# reusable function for reading and visualizing images within a given directory\n\ndef read_and_visualize(data_path, n_images=3, image_size=(224, 224), columns=3, visualize=True):\n    \"\"\"\n    Function reads the images from the directory path provided, scoops their individual file_paths,\n    labels. It also performs resizing as well converting the color mode for quality visualization.\n    \n    This function is effective if the directory only contains two folders of the classes.\n    \n    Args:\n        data_path -> (str): the str or path-like var of the directory on which the activity is to be performed.\n        n_images -> (int): Defaults to 3, This represents the number of images to visualize, can be adjusted to fit desire.\n        image_size -> tuple(int, int): Defaults to (224, 224), this represents the desired image size on visualization.\n        columns -> (int): Defaults to 3, how many images shall be visualized per columns.\n        visualize -> (bool): Default to True, shows the visualizations otherwise No.\n    \n    Outputs:\n        image_paths -> List: This contains all the individual image paths for the visualized images.\n        labels -> List: This contains all the labels of the individual image paths for the visualized images.\n    \"\"\"\n    \n    image_paths = []\n    labels = []\n    \n    classes = os.listdir(data_path)\n    \n    for cls_ in classes:\n        class_path = os.path.join(data_path, cls_)\n        if not os.path.isdir(class_path):\n            continue\n        \n        class_image_paths = [os.path.join(class_path, im) for im in os.listdir(class_path) \n                             if f.lower().endswith(('png', 'jpeg', 'tiff'))][:n_images // len(classes)]\n        image_paths.extend(class_image_paths)\n        labels.extend([cls_] * len(class_image_paths))\n        \n    if visualize:\n        rows = (len(image_paths) + columns - 1) // columns\n        fig, axs = plt.subplots(rows, columns, figsize=(15, 5*rows))\n        \n        for idx, image_path in enumerate(image_paths):\n            image = Image.open(image_path).convert('RGB') # convert all images to RGB color mode\n            image = image.resize(image_size)\n            row = idx // columns\n            col = idx % columns\n            ax[row, col].imshow(image)\n            ax[row, col].set_title(f\"State of Brain: {classes[idx // (n_images // len(classes))]}, Image: {idx}\")\n            ax[row, col].axis('off')\n            \n        for j in range(len(image_paths_list), row * columns):\n            fig.delaxes(axs[j // columns, j % columns])\n            \n        plt.tight_layout()\n        plt.show()\n        ","metadata":{},"execution_count":null,"outputs":[]}]}